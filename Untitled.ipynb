{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: readability-lxml in /anaconda3/lib/python3.6/site-packages (0.7)\n",
      "Requirement already satisfied: chardet in /anaconda3/lib/python3.6/site-packages (from readability-lxml) (3.0.4)\n",
      "Requirement already satisfied: cssselect in /anaconda3/lib/python3.6/site-packages (from readability-lxml) (1.0.3)\n",
      "Requirement already satisfied: lxml in /anaconda3/lib/python3.6/site-packages (from readability-lxml) (4.2.1)\n",
      "[nltk_data] Downloading package punkt to /Users/henry/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading averaged_perception_tagger: Package\n",
      "[nltk_data]     'averaged_perception_tagger' not found in index\n",
      "[nltk_data] Downloading package tagsets to /Users/henry/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/henry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install readability-lxml\n",
    "\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import requests\n",
    "import bs4\n",
    "import nltk\n",
    "import codecs\n",
    "from nltk.corpus import stopwords\n",
    "from readability.readability import Document\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perception_tagger')\n",
    "nltk.download('tagsets')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/henry/Desktop/digital_humanities'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=os.getcwd()\n",
    "HTML_CORPUS=os.path.join(BASE, 'data', 'raw', 'corpus2')\n",
    "\n",
    "def fetch(url):\n",
    "    res = requests.get(url)\n",
    "    return res.text\n",
    "\n",
    "def save_webpath(url, category=None, encoding='utf-8'):\n",
    "    if category:\n",
    "        dpath = os.path.join(HTML_CORPUS, category)\n",
    "    else:\n",
    "        dpath = HTML_CORPUS\n",
    "        \n",
    "    if not os.path.exists(dpath):\n",
    "        os.makedirs(dpath)\n",
    "        \n",
    "    file_name = os.path.basename(url)\n",
    "    path= os.path.join(dpath, file_name + '.html')\n",
    "    \n",
    "    if os.path.exists(path):\n",
    "        raise ValueError('warning, file with that name aleady exists')\n",
    "    else:\n",
    "        with open (path, 'w', encoding = encoding) as f:\n",
    "            f.write(fetch(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.theguardian.com/commentisfree/2018/dec/20/government-stupid-labour-brexit-referendum-jeremy-corbyn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_webpath(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE=os.getcwd()\n",
    "DATA = os.path.join(BASE, 'data')\n",
    "NEW_CORPUS = os.path.join(DATA,'interim', 'new_corpus')\n",
    "CORPUS = os.path.join(DATA, 'raw', 'corpus2')\n",
    "\n",
    "TAGS=['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "def preprocess(path):\n",
    "    with open(path, 'r') as f:\n",
    "        html = Document(f.read()).summary()\n",
    "        soup = bs4.BeautifulSoup(html, features = 'lxml')\n",
    "        for tag in soup.find_all(TAGS):\n",
    "            paragraph = tag.get_text()\n",
    "            \n",
    "            yield[\n",
    "                nltk.pos_tag(nltk.wordpunct_tokenize(sentence))\n",
    "                for sentence in nltk.sent_tokenize(paragraph)\n",
    "            ]\n",
    "            \n",
    "def transform(htmldir, textdir):\n",
    "    if not os.path.exists(textdir):\n",
    "        os.makedirs(textdir)\n",
    "        \n",
    "    for name in os.listdir(htmldir):\n",
    "        try:\n",
    "            inpath = os.path.join(htmldir, name)\n",
    "            outpath = os.path.join(textdir, os.path.splitext(name)[0] + '.txt')\n",
    "            \n",
    "            if os.path.isfile(inpath):\n",
    "                with open (outpath, 'w+', encoding = 'utf-8') as f:\n",
    "                    for paragraph in preprocess(inpath):\n",
    "                        for sentence in paragraph:\n",
    "                            f.write(\" \".join(f\"{word}/{tag}\" for word, tag in sentence))\n",
    "                            f.write(\"\\n\")\n",
    "                        f.write(\"\\n\")\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f'could not parse HTML: {e} ')\n",
    "            continue\n",
    "            \n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    transform(CORPUS, NEW_CORPUS)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_CORPUS = os.path.join(DATA, 'interim', 'new_corpus')\n",
    "corpus = nltk.corpus.TaggedCorpusReader(NEW_CORPUS, r\".*\\.txt\")\n",
    "\n",
    "words=corpus.words()\n",
    "tagged_words= corpus.tagged_words()\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_counts(words):\n",
    "    counts = Counter()\n",
    "    tokens = Counter()\n",
    "    for word in words:\n",
    "        counts['words'] += 1\n",
    "        tokens[word] += 1\n",
    "    #return{'There are '+str(counts['words'])+' words and '+ str(len(tokens))+' unique tokens in the corpus.'+' The lexical diversity is '+ str(counts['words'] / len(tokens))  }\n",
    "    print('There are '+str(counts['words'])+' words and '+ str(len(tokens))+' unique tokens in the corpus.'+' The lexical diversity is '+ str(counts['words'] / len(tokens)))\n",
    "\n",
    "def five_most_common_word_classes(tagged_words):\n",
    "    counts = Counter()\n",
    "    for word, tag in tagged_words:\n",
    "        counts[tag]+=1\n",
    "    counts5=counts.most_common(5)\n",
    "    print(\"\\n Here are the five most common word classes:\")\n",
    "    for n, tag in enumerate(counts5):\n",
    "        print(f\" {n + 1}. {tag} \")\n",
    "\n",
    "from nltk import ngrams\n",
    "def ten_most_common_unigrams(words_for_ngrams):\n",
    "    words_for_ngrams=[]\n",
    "    for word in words:\n",
    "        word=word.lower()\n",
    "        if word not in stop_words:\n",
    "            if word.isalpha():\n",
    "                words_for_ngrams.append(word)\n",
    "              \n",
    "    unigrams=Counter()\n",
    "    unigrams = ngrams(words_for_ngrams,1)\n",
    "    bigrams = ngrams(words_for_ngrams,2)\n",
    "    unigrams2=Counter(unigrams)\n",
    "    bigrams2=Counter(bigrams)\n",
    "    \n",
    "    print(\"\\n Here are the ten most common unigrams:\")\n",
    "    for n, word in enumerate(unigrams2.most_common(10)):\n",
    "        print(f\"    {n + 1}. {word}\")\n",
    "        \n",
    "    print(\"\\n Here are the ten most common bigrams:\")\n",
    "    for n, word in enumerate(bigrams2.most_common(10)):\n",
    "        print(f\"    {n + 1}. {word}\" )\n",
    "        \n",
    "        \n",
    "def noun_to_verb_ratio(corpus):\n",
    "    tag_counts=Counter()\n",
    "    for word, tag in tagged_words:\n",
    "        if tag.startswith('N'):\n",
    "            tag_counts['nouns'] += 1\n",
    "        if tag.startswith('V'):\n",
    "            tag_counts['verbs']+=1\n",
    "    number_of_nouns=tag_counts['nouns']\n",
    "    number_of_verbs=tag_counts['verbs']\n",
    "    n_to_v_ratio=number_of_nouns/(number_of_nouns+number_of_verbs)\n",
    "    #return{'There are '+str(number_of_nouns)+' nouns and '+str(number_of_verbs)+\" verbs in the corpus. The noun-to-verb ratio is \"+str(n_to_v_ratio)}\n",
    "    print('There are '+str(number_of_nouns)+' nouns and '+str(number_of_verbs)+\" verbs in the corpus. The noun-to-verb ratio is \"+str(n_to_v_ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1122 words and 553 unique tokens in the corpus. The lexical diversity is 2.0289330922242317\n",
      "\n",
      " Here are the five most common word classes:\n",
      " 1. ('NN', 200) \n",
      " 2. ('IN', 118) \n",
      " 3. ('DT', 114) \n",
      " 4. ('NNP', 89) \n",
      " 5. ('JJ', 78) \n",
      "\n",
      " Here are the ten most common unigrams:\n",
      "    1. (('stupid',), 10)\n",
      "    2. (('labour',), 10)\n",
      "    3. (('corbyn',), 6)\n",
      "    4. (('government',), 5)\n",
      "    5. (('tory',), 5)\n",
      "    6. (('brexit',), 5)\n",
      "    7. (('cabinet',), 5)\n",
      "    8. (('may',), 5)\n",
      "    9. (('would',), 4)\n",
      "    10. (('referendum',), 4)\n",
      "\n",
      " Here are the ten most common bigrams:\n",
      "    1. (('leave', 'voters'), 3)\n",
      "    2. (('panic', 'buying'), 2)\n",
      "    3. (('three', 'day'), 2)\n",
      "    4. (('day', 'week'), 2)\n",
      "    5. (('voters', 'election'), 2)\n",
      "    6. (('liberal', 'democrats'), 2)\n",
      "    7. (('would', 'rise'), 2)\n",
      "    8. (('support', 'among'), 2)\n",
      "    9. (('britain', 'stupidest'), 1)\n",
      "    10. (('stupidest', 'hour'), 1)\n",
      "There are 340 nouns and 148 verbs in the corpus. The noun-to-verb ratio is 0.6967213114754098\n"
     ]
    }
   ],
   "source": [
    "get_counts(words)\n",
    "five_most_common_word_classes(tagged_words)\n",
    "ten_most_common_unigrams(words)\n",
    "noun_to_verb_ratio(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
