What are your humanities research questions?

how many words and unique tokens are in this report?
what are the five most common word classes?
what are the ten most common unigrams and bigrams?
how many nouns and verbs are in this report? 
what is the noun to verb ration?


Which data did you use?
url="https://www.theguardian.com/commentisfree/2018/dec/20/government-stupid-labour-brexit-referendum-jeremy-corbyn"


What did you do to the data (and how can I reproduce it)?
it can be reproduced by running the code. 


What does the analysis show, how does it answer the humanities research question?

There are 1495 words and 564 unique tokens in the corpus. The lexical diversity is 2.650709219858156

 Here are the five most common word classes:
 1. ('NN', 211) 
 2. ('NNP', 162) 
 3. ('IN', 158) 
 4. ('DT', 140) 
 5. ('JJ', 94) 

 Here are the ten most common unigrams:
    1. (('may',), 23)
    2. (('said',), 23)
    3. (('minister',), 13)
    4. (('prime',), 12)
    5. (('would',), 11)
    6. (('eu',), 10)
    7. (('deal',), 10)
    8. (('mps',), 9)
    9. (('brexit',), 8)
    10. (('backstop',), 6)

 Here are the ten most common bigrams:
    1. (('prime', 'minister'), 12)
    2. (('theresa', 'may'), 5)
    3. (('eu', 'leaders'), 4)
    4. (('legal', 'guarantee'), 3)
    5. (('brexit', 'deal'), 3)
    6. (('irish', 'backstop'), 2)
    7. (('tory', 'mps'), 2)
    8. (('confidence', 'vote'), 2)
    9. (('rudd', 'said'), 2)
    10. (('mps', 'back'), 2)
There are 428 nouns and 298 verbs in the corpus. The noun-to-verb ratio is 0.5895316804407713

From these outputs we can find that the prime minister Theresa May is the key person of Brexit. 


Critically analyse your own pipeline for potential bias and problems. What would still need to be done for the analysis to be trustable?

we need to collect and analyze more articles to obtain a wider and deeper interpretation. 

